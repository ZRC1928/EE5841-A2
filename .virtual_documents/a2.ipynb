


# === Imports ===
import os, random
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from sklearn.datasets import fetch_openml
from sklearn.model_selection import train_test_split
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.pipeline import Pipeline

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers


# === Reproducibility ===
SEED = 42
os.environ["PYTHONHASHSEED"] = str(SEED)
random.seed(SEED)
np.random.seed(SEED)
tf.random.set_seed(SEED)

print("TensorFlow:", tf.__version__)








# Required access method:
adult = fetch_openml(name="adult", version=2, as_frame=True)

X = adult.data
y = adult.target  # strings: '>50K' or '<=50K'

print("X shape:", X.shape)
print("y shape:", y.shape)
X.head()











# TODO: Use a 60/20/20 split (train/val/test), stratified by label.

# Step 1: Split the data into training (60%) and temporary (40%) sets
X_train, X_temp, y_train, y_temp = train_test_split(
    X, y,
    test_size=0.4 ,            # TODO: fraction for validation+test
    random_state=SEED ,         # TODO: set the random seed
    stratify=y               # TODO: stratify by the target labels
)

# Step 2: Split the temporary set equally into validation (20%) and test (20%)
X_val, X_test, y_val, y_test = train_test_split(
    X_temp, y_temp,
    test_size=0.5 ,            # TODO: fraction for test split
    random_state=SEED ,         # TODO: set the same random seed
    stratify=y_temp               # TODO: stratify by the temporary labels
)

# Verify split sizes
print("Train:", X_train.shape, "Val:", X_val.shape, "Test:", X_test.shape)









# TODO: Identify categorical and numerical feature columns from the training set
cat_cols = X_train.select_dtypes(include=['object' , 'category']).columns.tolist()

num_cols = X_train.select_dtypes(exclude=['object' , 'category']).columns.tolist()

print("Categorical columns:", len(cat_cols))
print("Numeric columns:", len(num_cols))


# TODO: Build a preprocessing pipeline
# - Apply StandardScaler to numerical features
# - Apply OneHotEncoder to categorical features

preprocess = ColumnTransformer(
    transformers=[
        ("num", StandardScaler(), num_cols),   # TODO: scaler for numerical columns
        ("cat", OneHotEncoder(
            handle_unknown="ignore",
            sparse_output=False        # TODO: return dense output
        ), cat_cols),
    ],
    remainder='drop'                  # TODO: drop unused columns
)


# TODO: Fit preprocessing on TRAIN only, then transform all splits
X_train_p = preprocess.fit_transform(X_train)
X_val_p   = preprocess.transform(X_val)
X_test_p  = preprocess.transform(X_test)


# TODO: Encode labels into {0, 1}
# 1 corresponds to income >50K
y_train_i = (y_train == "50k").astype("int32").to_numpy()
y_val_i   = (y_val   == "50k").astype("int32").to_numpy()
y_test_i  = (y_test  == "50k").astype("int32").to_numpy()

# Verify processed feature shapes
print("Processed shapes:", X_train_p.shape, X_val_p.shape, X_test_p.shape)









# TODO: Complete the deep MLP builder with configurable depth, width, and regularization

def build_deep_mlp(
    input_dim: int,
    depth: int = 8,                     # TODO: number of hidden layers
    width: int = 16,                     # TODO: number of units per layer
    activation: str = "relu",                # TODO: hidden-layer activation
    kernel_initializer: str | keras.initializers.Initializer = "random_normal",
    use_batchnorm: bool = True,             # TODO: enable/disable BatchNorm
    dropout_rate: float = 0.3,             # TODO: dropout probability
    l2_weight: float = 0.01,                # TODO: L2 regularization strength
) -> keras.Model:
    """Deep MLP for binary classification with configurable init, BN, dropout, and L2."""

    # TODO: Add L2 regularization only if l2_weight > 0
    reg = keras.regularizers.L2(l2_weight) if l2_weight and l2_weight > 0 else None

    # TODO: Define model input
    inputs = keras.Input(shape=(input_dim ,))
    x = inputs

    # TODO: Build hidden layers
    for _ in range(depth):
        x = layers.Dense(
            width,                           # TODO: layer width
            activation=None,               # TODO: no activation here
            kernel_initializer=kernel_initializer,
            kernel_regularizer=reg ,
        )(x)

        # TODO: Optional Batch Normalization
        if use_batchnorm:
            x = layers.BatchNormalization()(x)

        # TODO: Apply activation
        x = layers.Activation(activation)(x)

        # TODO: Optional Dropout
        if dropout_rate and dropout_rate > 0:
            x = layers.Dropout (dropout_rate)(x)

    # TODO: Output layer for binary classification
    outputs = layers.Dense(1, activation="sigmoid")(x)

    # TODO: Build and return the model
    model = keras.Model(inputs, outputs)
    return model

# function call- do not change
input_dim = X_train_p.shape[1]
deep_model = build_deep_mlp(input_dim=input_dim, depth=6, width=256)
deep_model.summary()









# TODO: Complete the utility that computes layer-wise gradient norms for Dense kernels

def layerwise_grad_norms(model: keras.Model, x_batch, y_batch):
    # TODO: convert x_batch to a float32 tensor
    x_batch = tf.convert_to_tensor(x_batch, dtype="f")

    # TODO: reshape y_batch to (N, 1) and convert to float32 tensor
    y_batch = tf.convert_to_tensor(y_batch.reshape(-1, _____), dtype="f")

    # TODO: compute loss under a GradientTape context
    with tf.GradientTape() as tape:
        y_pred = model(x_batch, training=True)   # TODO: training flag
        loss = keras.losses.binary_crossentropy(y_batch, y_pred)  # TODO: binary cross-entropy
        loss = tf.reduce_mean(_____)

    # TODO: compute gradients of loss w.r.t. trainable variables
    grads = tape.gradient(_____, model.trainable_variables)

    # TODO: collect gradient norms for Dense layer kernels only
    norms = []
    names = []
    for var, g in zip(model.trainable_variables, grads):
        if g is None:
            continue
        if "dense" in var.name and "kernel" in var.name:
            norms.append(float(tf.norm(_____)))   # TODO: gradient tensor
            names.append(_____ )                  # TODO: variable name

    return float(loss), names, norms


# TODO: Prepare a mini-batch for gradient diagnostics
BATCH = _____
x_b = X_train_p[:_____]
y_b = y_train_i[:_____]


# TODO: Create a model with a clearly "naïve" initialization for Q1
naive_init = keras.initializers.RandomNormal(
    mean=_____,
    stddev=_____,
    seed=_____
)

model_naive = build_deep_mlp(
    input_dim=_____,
    depth=_____,
    width=_____,
    activation=_____,
    kernel_initializer=_____
)

# TODO: Run a forward pass once to build weights
_ = model_naive(tf.convert_to_tensor(_____, dtype=_____))

# function call- do not change
loss0, names0, norms0 = layerwise_grad_norms(model_naive, x_b, y_b)
print("Initial batch loss:", loss0)
for n, v in list(zip(names0, norms0))[:6]:
    print(n, "->", v)









# TODO: Complete helper functions to compile and train a Keras model

def compile_model(model: keras.Model, optimizer: keras.optimizers.Optimizer):
    model.compile(
        optimizer='SGD',                         # TODO: pass the optimizer
        loss='binary_crossentropy',                              # TODO: binary classification loss
        metrics=[keras.metrics.Accuracy(name="accuracy")  # TODO: accuracy metric + name
               ]
    )
    return model


def fit_model(model: keras.Model, epochs: int = 20, batch_size: int = 256):
    history = model.fit(
        X_train_p, y_train_i,                            # TODO: training features and labels
        validation_data=(X_val_p, y_val_i),           # TODO: validation features and labels
        epochs=epochs,
        batch_size=batch_size,
        verbose=0                            # TODO: 0 for silent training
    )
    return history


# TODO: Define Xavier (Glorot) and He initializations
xavier = keras.initializers.GlorotNormal(seed=SEED)
he     = keras.initializers.HeNormal(seed=SEED)


# TODO: Build + compile two controlled models:
# 1) Xavier + tanh, 2) He + ReLU. Use the same optimizer type and learning rate.
model_xavier = compile_model(
    build_deep_mlp(
        input_dim=X_train_p.shape[1],
        depth=8,
        width=16,
        activation='tanh',                        # TODO: tanh
        kernel_initializer=xavier
    ),
    keras.optimizers.SGD(learning_rate=0.01)  # TODO: SGD and lr
)

model_he = compile_model(
    build_deep_mlp(
        input_dim=X_train_p.shape[1],
        depth=8,
        width=16,
        activation='relu',                        # TODO: relu
        kernel_initializer=he
    ),
    keras.optimizers.SGD(learning_rate=0.01)  # TODO: same optimizer and lr
)

# function call- do not change
hist_xavier = fit_model(model_xavier, epochs=20)
hist_he = fit_model(model_he, epochs=20)

plt.figure()
plt.plot(hist_xavier.history["val_loss"], label="Xavier (tanh) val_loss")
plt.plot(hist_he.history["val_loss"], label="He (relu) val_loss")
plt.xlabel("Epoch")
plt.ylabel("Validation Loss")
plt.legend()
plt.title("Initialization comparison (controlled)")
plt.show()









# TODO: Regularization Experiments — L2 Weight Decay and Dropout

# TODO: Choose ONE best initialization from Q2 and keep it fixed
base_init = xavier


# =========================
# L2 Weight Decay Study
# =========================

model_l2_1 = compile_model(
    build_deep_mlp(
        input_dim=X_train_p.shape[1],
        depth=8,
        width=16,
        activation='tanh',                 # TODO: activation function
        kernel_initializer=base_init,
        l2_weight=1e-4                   # TODO: smaller L2 value (e.g., 1e-4)
    ),
    keras.optimizers.SGD(              # TODO: optimizer type
        learning_rate=0.01,
        momentum=0.0                   # TODO: momentum value
    )
)

model_l2_2 = compile_model(
    build_deep_mlp(
        input_dim=X_train_p.shape[1],
        depth=8,
        width=16,
        activation='tanh',
        kernel_initializer=base_init,
        l2_weight=1e-3                   # TODO: larger L2 value (e.g., 1e-3)
    ),
    keras.optimizers.SGD(
        learning_rate=0.01,
        momentum=0.0
    )
)

# function call- do not change
hist_l2_1 = fit_model(model_l2_1, epochs=20)
hist_l2_2 = fit_model(model_l2_2, epochs=20)


# =========================
# Dropout Study
# =========================

model_do_1 = compile_model(
    build_deep_mlp(
        input_dim=X_train_p.shape[1],
        depth=8,
        width=16,
        activation='tanh',
        kernel_initializer=base_init,
        dropout_rate=0.2               # TODO: moderate dropout (e.g., 0.2)
    ),
    keras.optimizers.SGD(
        learning_rate=0.01,
        momentum=0.0
    )
)

model_do_2 = compile_model(
    build_deep_mlp(
        input_dim=X_train_p.shape[1],
        depth=8,
        width=16,
        activation='tanh',
        kernel_initializer=base_init,
        dropout_rate=0.5               # TODO: stronger dropout (e.g., 0.5)
    ),
    keras.optimizers.SGD(
        learning_rate=0.01,
        momentum=0.0
    )
)

# function call- do not change
hist_do_1 = fit_model(model_do_1, epochs=20)
hist_do_2 = fit_model(model_do_2, epochs=20)

plt.figure()
plt.plot(hist_l2_1.history["val_loss"], label="L2=1e-4 val_loss")
plt.plot(hist_l2_2.history["val_loss"], label="L2=1e-3 val_loss")
plt.plot(hist_do_1.history["val_loss"], label="Dropout=0.2 val_loss")
plt.plot(hist_do_2.history["val_loss"], label="Dropout=0.5 val_loss")
plt.xlabel("Epoch")
plt.ylabel("Validation Loss")
plt.legend()
plt.title("Regularization comparison (L2 vs Dropout)")
plt.show()









# TODO: BatchNorm Experiment — keep initialization + optimizer controlled

# Build + compile model WITH Batch Normalization
model_bn = compile_model(
    build_deep_mlp(
        input_dim=X_train_p.shape[1],
        depth=8,
        width=16,
        activation='tanh',                  # TODO: activation function
        kernel_initializer=base_init,          # TODO: base_init
        use_batchnorm=True                # TODO: enable BN
    ),
    keras.optimizers.SGD(
        learning_rate=1e-2,
        momentum=0.0                     # TODO: keep momentum fixed
    )
)

# Build + compile model WITHOUT Batch Normalization
model_no_bn = compile_model(
    build_deep_mlp(
        input_dim=X_train_p.shape[1],
        depth=8,
        width=16,
        activation='tanh',
        kernel_initializer=base_init,
        use_batchnorm=False                # TODO: disable BN
    ),
    keras.optimizers.SGD(
        learning_rate=1e-2,
        momentum=0.0
    )
)

# function call- do not change
hist_bn = fit_model(model_bn, epochs=20)
hist_no_bn = fit_model(model_no_bn, epochs=20)

plt.figure()
plt.plot(hist_no_bn.history["val_loss"], label="No BN val_loss")
plt.plot(hist_bn.history["val_loss"], label="With BN val_loss")
plt.xlabel("Epoch")
plt.ylabel("Validation Loss")
plt.legend()
plt.title("Batch Normalization effect on convergence")
plt.show()









# TODO: Optimizer Comparison — keep architecture + initialization fixed

# TODO: Create a shared architecture config dictionary (same for all optimizers)
arch_kwargs = dict(
    input_dim=X_train_p.shape[1],
    depth=8,
    width=16,
    activation='tanh',
    kernel_initializer=base_init,      # TODO: base_init from earlier
    use_batchnorm=True            # TODO: keep BN fixed (True or False)
)

# TODO: Build + compile models with different optimizers (architecture must be identical)

model_sgd = compile_model(
    build_deep_mlp(**arch_kwargs),
    keras.optimizers.SGD(
        learning_rate=1e-2,
        momentum=0.0
    )
)

model_mom = compile_model(
    build_deep_mlp(**arch_kwargs),
    keras.optimizers.SGD(
        learning_rate=1e-2,
        momentum=0.9
    )
)

model_adam = compile_model(
    build_deep_mlp(**arch_kwargs),
    keras.optimizers.Adam(
        learning_rate=1e-3
    )
)

# function call- do not change
hist_sgd = fit_model(model_sgd, epochs=20)
hist_mom = fit_model(model_mom, epochs=20)
hist_adam = fit_model(model_adam, epochs=20)

plt.figure()
plt.plot(hist_sgd.history["val_loss"], label="SGD val_loss")
plt.plot(hist_mom.history["val_loss"], label="SGD+Momentum val_loss")
plt.plot(hist_adam.history["val_loss"], label="Adam val_loss")
plt.xlabel("Epoch")
plt.ylabel("Validation Loss")
plt.legend()
plt.title("Optimizer comparison")
plt.show()









def evaluate(model: keras.Model, X, y):
    y = y.reshape(-1, 1).astype("float32")
    return dict(zip(model.metrics_names, model.evaluate(X, y, verbose=0)))

# TODO: Choose the best model based on your controlled experiments.
final_model = model_adam

val_results = evaluate(final_model, X_val_p, y_val_i)
test_results = evaluate(final_model, X_test_p, y_test_i)

print("Validation:", val_results)
print("Test:", test_results)


# TODO: Complete the evaluation utility and final model selection

def evaluate(model: keras.Model, X, y):
    # TODO: reshape labels to (N, 1) and cast to float32
    y = y.reshape(-1, _____).astype('f')

    # TODO: evaluate the model silently and return a dictionary of metrics
    return dict(
        zip(
            model._____,                   # TODO: metric names
            model.compile(X, y, verbose=0)
        )
    )


# TODO: Select the best-performing model based on your controlled experiments
# (initialization, regularization, BatchNorm, and optimizer)
final_model = _____


# TODO: Evaluate the final model on validation and test sets
val_results  = evaluate(_____, _____, _____)
test_results = evaluate(_____, _____, _____)

print("Validation:", val_results)
print("Test:", test_results)



















